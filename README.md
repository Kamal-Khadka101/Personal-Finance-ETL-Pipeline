# ğŸ’° Personal Finance ETL Pipeline & Dashboard

A complete end-to-end data pipeline for analyzing personal bank statements. Automatically extracts, transforms, and loads transaction data into PostgreSQL, with an interactive Streamlit dashboard for visualization.

## ğŸ¯ Features

- **ğŸ”„ Automated ETL Pipeline**: Drop CSV files, watch them auto-process
- **ğŸ§¹ Smart Data Cleaning**: Automatic categorization of transactions
- **ğŸ’¾ PostgreSQL Storage**: Persistent database with optimized queries
- **ğŸ“Š Interactive Dashboard**: Beautiful visualizations with Streamlit
- **ğŸ¨ Rich Analytics**: Spending breakdowns, trends, and insights
- **ğŸ” Advanced Filtering**: Filter by date, category, transaction type
- **ğŸ“¥ Data Export**: Download filtered data as CSV

## ğŸ—ï¸ Architecture

```
CSV Files  â†’  File Watcher  â†’  Transform  â†’  PostgreSQL  â†’  Dashboard
(Bank          (watch.py)      (Pandas)       (Docker)      (Streamlit)
Statements)         â†“              â†“              â†“             â†“
                 Detects      Categorize     Store Data    Visualize
                 New Files    Clean Data     Optimize      Analyze
                              Validate       Index         Export
```

## ğŸ“‹ Prerequisites

- **Python 3.8+** - [Download](https://www.python.org/downloads/)
- **Docker Desktop** - [Download](https://www.docker.com/products/docker-desktop/)
- **Git** (for cloning) - [Download](https://git-scm.com/downloads)

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/finance-etl-pipeline.git
cd finance-etl-pipeline
```

### 2. Set Up Python Environment

```bash
# Create virtual environment
python -m venv venv

# Activate it
# On Linux/Mac:
source venv/bin/activate
# On Windows:
venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configure Environment Variables (Optional)

```bash
# Copy the example environment file
cp .env.example .env

# Edit .env with your settings if needed
nano .env
```

### 4. Start PostgreSQL Database

```bash
# Start Docker container
docker compose up -d

# Wait for initialization (first time only)
sleep 15

# Verify database is running
python test_database.py
```

### 5. Start the File Watcher (Terminal 1)

```bash
python watch.py
```

### 6. Start the Dashboard (Terminal 2)

```bash
streamlit run dashboard.py
```

Dashboard opens at: **http://localhost:8501**

### 7. Process Your First CSV

```bash
# Drop a CSV file
cp sample_data/MOCK_DATA.csv finance/watch/
```

## ğŸ“‚ Project Structure

```
finance-etl-pipeline/
â”œâ”€â”€ config.py              # Configuration & categorization rules
â”œâ”€â”€ transform.py           # Data cleaning & transformation logic
â”œâ”€â”€ database.py            # PostgreSQL connection & operations
â”œâ”€â”€ watch.py               # File watcher (main ETL orchestrator)
â”œâ”€â”€ dashboard.py           # Streamlit visualization app
â”œâ”€â”€ test_database.py       # Database connection testing
â”œâ”€â”€ docker-compose.yml     # PostgreSQL container setup
â”œâ”€â”€ init.sql               # Database schema initialization
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env.example           # Environment variables template
â”œâ”€â”€ .gitignore            # Git ignore rules
â”œâ”€â”€ README.md             # This file
â”‚
â”œâ”€â”€ finance/
â”‚   â”œâ”€â”€ watch/            # Drop CSV files here
â”‚   â”œâ”€â”€ processed/        # Successfully processed files
â”‚   â””â”€â”€ failed/           # Failed files with error logs
â”‚
â””â”€â”€ sample_data/          # Example CSV files
    â””â”€â”€ MOCK_DATA.csv
```

## ğŸ“Š CSV File Format

Your CSV must have these columns:

| Column             | Description                    | Example              |
|--------------------|--------------------------------|----------------------|
| `date`             | Transaction date (MM/DD/YYYY)  | 01/15/2024           |
| `description`      | Transaction description        | STARBUCKS STORE #123 |
| `category`         | Original category (optional)   | Dining               |
| `transaction_type` | Original type (optional)       | Debit                |
| `amount`           | Transaction amount             | -5.75                |

## ğŸ¨ Customizing Categories

Edit `config.py`:

```python
CATEGORY_PATTERNS = {
    'Income': re.compile(r'PAYCHECK|SALARY', re.IGNORECASE),
    'Food & Dining': re.compile(r'STARBUCKS|CHIPOTLE', re.IGNORECASE),
    'Your Category': re.compile(r'KEYWORD1|KEYWORD2', re.IGNORECASE),
}
```

## ğŸ—„ï¸ Database Operations

```bash
# Connect to PostgreSQL
docker exec -it finance_db psql -U postgres -d finance_db

# Query data
SELECT category, COUNT(*), SUM(amount) FROM transactions GROUP BY category;

# Exit
\q
```

## ğŸ§ª Testing

```bash
# Test database
python test_database.py

# Test transformation
python test_manual.py
```

## ğŸ› Troubleshooting

**"Cannot connect to PostgreSQL"**
```bash
docker compose ps
docker compose restart
```

**"No transactions in dashboard"**
```bash
docker exec -it finance_db psql -U postgres -d finance_db -c "SELECT COUNT(*) FROM transactions;"
```

**Dashboard not updating**
- Click "Rerun" (top-right)
- Press `R` key

## ğŸ”’ Security Notes

âš ï¸ **For local development only** - Uses simplified security. For production:
- Use environment variables
- Enable SSL
- Implement authentication

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Streamlit](https://streamlit.io/)
- [PostgreSQL](https://www.postgresql.org/)
- [Plotly](https://plotly.com/)
- [Pandas](https://pandas.pydata.org/)

---

**Made with â¤ï¸ for better financial insights**
